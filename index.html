<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:title" content="Extreme Rotation Estimation in the Wild" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"
    integrity="sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> 
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="style.css">
  <title>Extreme Rotation Estimation in the Wild</title>
  <style>
  	.teaser {
  		position: relative;
  		display: inline;
  	}
  	.teaser .img-top {
  		position: absolute;
  		top: 0;
  		left: 0;
  		opacity: 0;
  	}
  	.teaser:hover .img-top { opacity: 1; }
         .tab { margin-left: 40px; }
		 

  	.dataset_desc {
        position: relative;
        top: 0;
        bottom: 0;
        left: 0;
        right: 0;
        background: #fff;
        color: #000;
        visibility: hidden;
        opacity: 0;

        /* transition effect. not necessary */
        transition: opacity .2s, visibility .2s;

        margin: auto;
        font-family: monospace;
        text-align: center;
        line-height: 130px;
        font-size: 15pt;
		width: 350px;
		

    }

    .dataset:hover .dataset_desc {
        visibility: visible;
        opacity: 0.8;
    }

    .dataset_img {
        width: 300px;
    }

  </style>

</head>

<body class="container" style="max-width:840px">

  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
    integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-Piv4xVNRyMGpqkS2by6br4gNJ7DXjqk09RmUpJ8jgGtD7zP9yug3goQfGII0yAns"
    crossorigin="anonymous"></script>

  <!-- heading -->
  <div>

    <!-- title -->
    <div class='row mt-5 mb-3'>
      <div class='col text-center'>
        <p class="h1 font-weight-normal">Extreme Rotation Estimation in the Wild</p>
      </div>
    </div>

    <!-- authors -->
    <div class="col text-center h6 font-weight-bold mb-2 ">
      <span class="col-md-4 col-xs-6 pb-2"><a href="">Hana Bezalel</a><sup>1</sup></span>
	  <span class="col-md-4 col-xs-6 pb-2" ><a href="">Dotan Ankri</a><sup>1</sup></span>
	  <span class="col-md-4 col-xs-6 pb-2" ><a href="https://www.cs.cornell.edu/~ruojin/">Ruojin Cai</a><sup>2</sup></span>
      <span class="col-md-4 col-xs-6 pb-2"><a href="https://www.elor.sites.tau.ac.il/">Hadar Averbuch-Elor</a><sup>1,2</sup></span>
    </div>
	

    <!-- affiliations -->
    <div class="col text-center h6 font-weight-bold mb-2 ">
      <span class="author-block"><sup>1</sup>Tel Aviv University,</span>
	  <span class="author-block"><sup>2</sup>Cornell University</span>
    </div>

    <!-- links -->
    <div class='row mb-4'>
      <div class='col text-center'>
        <a href="" target="_blank" class="btn btn-outline-primary" role="button">
          <i class="ai ai-arxiv"></i>
          arXiv
        </a>
        <a href="https://github.com/TAU-VAILab/ExtremeRotationsInTheWild.git" target="_blank" class="btn btn-outline-primary"
          role="button">
          <i class="fa fa-github"></i>
          Code
        </a>
		<a href="" target="_blank"
            class="btn btn-outline-primary" role="button">
            <i class="fa fa-database"></i>
            Dataset
        </a>
        <a href="https://mailtauacil-my.sharepoint.com/:u:/g/personal/hanahenb_mail_tau_ac_il/EU6X9dbvvxFKlNxzLV2hS10Bg0bAIFoPP08jGmNEfIIEsw?e=rGUrEf" target="_blank" class="btn btn-outline-primary" role="button">
            <i class="fa fa-eye"></i>
            Interactive Visualization
        </a>
      </div>
    </div>

    <!-- teaser -->
    <div class='row justify-content-center'>
		<div class="teaser">
                <img src="webpage_assets/teaser0001.png" class="img-fluid rounded mx-auto d-block" >
                <p class="img-top"> <img src="webpage_assets/teaser0050.png" class="img-fluid rounded mx-auto d-block" ></p>
        </div>
        <div class='text-center col-md-12 col-sm-12 col-xs-12 align-middle mt-1'>
          <p class='h6'>
            <em>Given a pair of images captured in the wild —
		     e.g. under arbitrary illumination and intrinsic camera parameters— and in extreme settings (with little or no overlap),
		     such as the images of the Dam Square in Amsterdam depicted in red and blue boxes above, 
		     can we leverage 3D priors to estimate the relative 3D rotation between the images? (Hover to see the scene) </em>
          </p>
          <hr>
        </div>
    </div>


    <!-- method -->

    <!-- viz -->
    <div class="row">
      <div class="col-md-12 col-sm-12 col-xs-12" >
        <p class="h4 font-weight-bold title" >Abstract</p>
        <p>
		  We present a technique and benchmark dataset for estimating the relative 3D orientation between a pair of 
		  Internet images captured in an extreme setting, where the images have limited or non-overlapping field of views. 
		  Prior work targeting extreme rotation estimation assume constrained 3D environments and emulate perspective images by cropping regions from panoramic views. 
		  However, real images captured in the wild are highly diverse, exhibiting variation in both appearance and camera intrinsics. 
		  In this work, we propose a Transformer-based method for estimating relative rotations in extreme real-world settings, 
		  and contribute the ExtremeLandmarkPairs dataset, assembled from scene-level Internet photo collections.
		  Our evaluation demonstrates that our approach succeeds in estimating the relative rotations in a wide variety of extreme-view Internet image pairs, 
		  outperforming various baselines, including dedicated rotation estimation techniques and contemporary 3D reconstruction methods. 

        </p>

        <hr>
      </div>
    </div>
	
	
	<!-- overview -->
	<div>
      <div class="row">
        <div class='col-md-12 col-sm-12 col-xs-12'>
          <p class='h4 font-weight-bold title'>Overview of our Method</p>
          <p class="ack">
				<div class='col-md-12 col-sm-12 col-xs-12 mt-3'>
                    <img src="webpage_assets/overview_new.JPG" class="img-fluid" alt="overview">
                </div> 
				We design a network architecture optimized for estimating 3D relative rotation from pairs of images captured in challenging, real-world conditions.
				Given a pair of input Internet images, we extract image features using pretrained LoFTR. These features
				are combined with auxiliary channels, including keypoint and pairwise matches masks, and  segmentation maps (visualized on the bottom
				left). These image features are reshaped into tokens and concatenated with Euler angle position embeddings, which are then processed by
				our Rotation Estimation Transformer module. The output Euler angle tokens and averaged image tokens are concatenated and processed
				by MLPs to predict the probability distribution of Euler angles, representing the relative 3D rotation between the input images.				
          </p>
        </div>
      </div>
      <hr>
    </div>
	<!-- Dataset  -->
	<div>
        <div class="row">
            <div class='col-md-9 col-sm-9 col-xs-12 justify'>
                <p class='h4 font-weight-bold'>ExtremeLandmarkPairs Dataset</p>
                <p>
                In this paper, we present a new approach that tackles the problem of extreme rotation estimation in the wild. 
				Internet (i.e., in the wild) images may vary due to a wide range of factors, including transient objects, 
				weather conditions, time of day, and the cameras’ intrinsic parameters. 
				To explore this problem, we introduce a new dataset, ExtremeLandmarkPairs(ELP), assembled from publicly-available scene-level 
				Internet image collections.  
				This dataset contains a training set with nearly 34K non-overlapping pairs originating from over 2K unique landmarks, constructed from <a href=https://megascenes.github.io/>MegaScenes</a> dataset. 
				Additionally, for evaluation, we have created two test sets, to separately examine image pairs captured in a single camera 
				setting with constant illumination (sELP)and image pairs captured in the wild (wELP).  These test sets are respectively sourced from the
				<a href=https://www.repository.cam.ac.uk/items/53788265-cb98-42ee-b85b-7a0cbc8eddb3/>Cambridge Landmarks</a> and 
			    <a href=https://www.cs.cornell.edu/projects/megadepth/>MegaDepth</a> datasets.
				ExtremeLandmarkPairs(ELP) dataset  can be accessed via <a href=https://www.cs.cornell.edu/projects/megadepth/>link</a>.
				
				Test sets statistics:
				</p>
				<div class='row justify-content-center'>
					<div class="dataset" style="width: 300px">
						<img src="webpage_assets/sELP_statistics.JPG" class="img-fluid rounded mx-auto d-block dataset_img" >
						<p class="dataset_desc" style="width: 300px">sELP</p>
					</div>
					<div class="dataset" style="width: 300px">
						<img src="webpage_assets/wELP_statistics.JPG" class="img-fluid rounded mx-auto d-block dataset_img" >
						<p class="dataset_desc" style="width: 300px">wELP</p>
					</div>
				</div>
  
            </div>
            <div class='col-md-3 col-sm-3 col-xs-12'>
                <div class="row mt-3">
                    <img src="webpage_assets/Dataset.JPG" alt="stroop effect example 2" class="img-thumbnail stroop-img"
                        width="100%">
                </div>
				<p  style="font-size: 14px;">
                In this figure, we can observe the camera distribution for the Vatican, Rome scene from the ExtremeLandmarkPairs Dataset
				We have constructed a dataset of real
				perspective image pairs with predominant rotational motion shown
				in (b) and (c) from the dense imagery reconstruction in (a).</p>
				
            </div>
        </div>
    </div>
	
	<!-- Results  -->
	<div>
	  <hr>
      <div class="row">
        <div class='col-md-12 col-sm-12 col-xs-12'>
          <p class='h4 font-weight-bold title'>Results</p>
          <p class="ack">
				See our <a href="https://mailtauacil-my.sharepoint.com/:f:/g/personal/hanahenb_mail_tau_ac_il/EpVO5rUC6NFFtuo7AYrc4A8BaET4N05mmyfedhWaVP2VUg?e=gef9Ew">interactive visualization</a> for results of all models on the sELP and wELP test sets.
				<div class='col-md-12 col-sm-12 col-xs-12 mt-3'>
                    <img src="webpage_assets/results_vis.JPG" class="img-fluid" alt="overview">
                </div>
				We visualize the results of our model over different overlap levels, where the images on the <font color=#FF7F50>left</font> serve as the reference points, 
				and their coordinate system detemines the relative rotation, which defines the images on the <font color=#6495ED>right</font>.
				The ellipsoids representing the ground truth are color-coded to match their respective images, with the estimated relative rotation illustrated
				by a <font color=#33F3FF>cyan</font> dashed line. As illustrated by the examples above, our method can accurately predict relative rotations for diverse image pairs
				containing varying appearances and intrinsic parameters.

          </p>
        </div>
      </div>
      <hr>
    </div>
    <!-- ack -->
    <div>
      <div class="row">
        <div class='col-md-12 col-sm-12 col-xs-12'>
          <p class='h4 font-weight-bold title'>Acknowledgements</p>
          <p class="ack">
		   This work was partially supported by ISF (grant number 2510/23).
          </p>
        </div>
      </div>
      <hr>
    </div>


    <!-- citation -->
    <div class="row">
      <div class="col-md-12 col-sm-12 col-xs-12">
        <p class="h4 font-weight-bold title">Citation</p>
        <pre><code>...</code></pre>
      </div>
    </div>

</body>

</html>
