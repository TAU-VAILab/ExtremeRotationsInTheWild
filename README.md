# Extreme Rotation Estimation in the Wild
This repository contains a PyTorch implementation of the paper:
> **Extreme Rotation Estimation in the Wild**<br>
> Hana Bezalel , Dotan Ankri, Ruojin Cai, Hadar Averbuch-Elor<br>
> Tel Aviv University<br>

>**Introduction** <br>
>We present a technique and benchmark dataset for estimating the relative 3D orientation between a pair of In
ternet images captured in an extreme setting, where the images have limited or non-overlapping field of views.
Prior work targeting extreme rotation estimation assume constrained 3D environments and emulate perspective images
by cropping regions from panoramic views. However, real images captured in the wild are highly diverse, exhibiting
variation in both appearance and camera intrinsics. In this work, we propose a Transformer-based method
for estimating relative rotations in extreme real-world settings, and contribute the ExtremeLandmarkPairs dataset,
assembled from scene-level Internet photo collections. Our evaluation demonstrates that our approach succeeds in
estimating the relative rotations in a wide variety of extreme-view Internet image pairs, outperforming various baselines,
including dedicated rotation estimation techniques and contemporary 3D reconstruction methods. We will release our
data, code, and trained models.
<p align="center">
<img src="g" width="90%"/>  
</p>
</br>

# Getting Started
